import ast
import json
import pdb
import re
import subprocess
import sys
import time
from enum import auto
from re import L
from turtle import pd

import datasets
import enchant
import numpy as np
import openai
import parsimonious
import requests
from prompt_library import (llm_similar_tasks, random_tasks,
                            similar_auto_breakdowns, similar_tasks)
from tqdm import tqdm
from utils import (Command, OpenAIModel, StacktraceItem, cache_dir, chunks,
                   get_subset, gpt3, parse_program, propose_decomposition,
                   propose_instruction, substring_match)
from pot_tools import safe_execute, simplify_ans

subscription_key = 'BING_API_KEY'
search_url = "https://api.bing.microsoft.com/v7.0/search"
headers = {"Ocp-Apim-Subscription-Key": subscription_key}
from serpapi import GoogleSearch
import time

serp_api_key = "SERP_API_KEY"


import re

# as per recommendation from @freylis, compile once only
CLEANR = re.compile('<.*?>') 

def cleanhtml(raw_html):
  cleantext = re.sub(CLEANR, '', raw_html)
  return cleantext

def search(query, top_k=1):
    params = {"q": query, "textDecorations": True, "textFormat": "HTML"}
    response = requests.get(search_url, headers=headers, params=params)
    response.raise_for_status()
    search_results = response.json()
    if "webPages" in search_results:
        snippets = [cleanhtml(v['name'] + " " + v['snippet']) for v in search_results['webPages']["value"]][:top_k]
    else:
        return "", None
    return "\n".join(snippets), None

def google_search(query, previous_input=None, top_k=1):
    params = {
      "q": query,
    #   "location": "Austin, Texas, United States",
      "hl": "en",
      "gl": "us",
      "google_domain": "google.com",
      "api_key": serp_api_key
    }

    search = GoogleSearch(params)
    try:
        res = search.get_dict()
    except:
        res = {}

    if 'answer_box' in res.keys() and 'answer' in res['answer_box'].keys():
        toret = res['answer_box']['answer']
    elif 'answer_box' in res.keys() and 'snippet' in res['answer_box'].keys():
        toret = res['answer_box']['snippet']
    elif 'answer_box' in res.keys() and 'snippet_highlighted_words' in res['answer_box'].keys():
        toret = res['answer_box']["snippet_highlighted_words"][0]
    elif 'organic_results' in res.keys() and 'snippet' in res["organic_results"][0].keys():
        toret= res["organic_results"][0]['snippet'] 
    else:
        toret = None

    return toret, None

def code_generate(instruction, code_input):
    # response = openai.Edit.create(
    # model="code-davinci-edit-001",
    # input="x = " + code_input,
    # instruction="Python code for " + instruction,
    # temperature=0,
    # top_p=1
    # )
    if instruction in code_input:
        # Code input is sufficient to write code
        comment = "\"\"\"\n{0}\nStore the final result as a variable named 'ans' and print it.\n\"\"\"\n\n".format(code_input)
    else:
        if re.search("#[0-9]+", instruction):
            input_string = re.search("#[0-9]+", instruction).group(0)
            instruction = instruction.replace(input_string, "Input")
        # comment = "\"\"\"\n{0}\nInput:{1}\nStore the final result as a variable named 'ans' and print it.\n\"\"\"\n\ndef".format(instruction, code_input)
        comment = "\"\"\"\n{0}\nInput:{1}\nStore the final result as a variable named 'ans' and print it.\n\"\"\"\n\n".format(instruction, code_input)

    response = openai.completions.create(
    model="gpt-3.5-turbo-instruct",
    # model="davinci-codex-002-msft",
    prompt=comment,
    temperature=0.0,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    stop=["print(ans)"]
    )
    
    code_snippet = response.choices[0].text
    time.sleep(5)

    # TODO: Add function run or remove the "def" entirely
    # return "def" + code_snippet, comment
    # if "ans" in code_snippet:
    #     code_snippet += "\nprint(ans)"

    # TODO: Add "print(ans) to the end."
    return code_snippet, comment

def code_execute(code_snippet, code_input=None):
    
    if code_input:
        try:
            # Run arithmetic python code generate
            code_output = safe_execute(code_input)
            result = simplify_ans(code_output)
            if result:
                return result, code_input
        except:
            pass

    try:
        # Run code snippet as is, if it was generated by code_generate
        # More robust outputs 
        result = subprocess.run([sys.executable, "-c", code_input], capture_output=True, text=True, timeout=60)
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        pass
    
    try:
        # Run arithmetic python code generate
        code_output = safe_execute(code_snippet)
        result = simplify_ans(code_output)
        if result:
            return result, code_snippet
    except:
        pass

    try:
        # Run code snippet as is, if it was generated by code_generate
        result = subprocess.run([sys.executable, "-c", code_snippet], capture_output=True, text=True, timeout=60)
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        pass

    if "error" in code_snippet or (code_input and "error" in code_input):
        return code_error(code_snippet, code_input)

    try:
        # Command instruction (if its code) starts in the next line.
        code_snippet = code_snippet.split("\n", 1)[1]
        result = subprocess.run([sys.executable, "-c", code_snippet], capture_output=True, text=True, timeout=60)
        if result.stderr == "":
            simplified_result = simplify_ans(result.stdout)
            return simplified_result, code_snippet
    except:
        return None, code_snippet

    return None, code_snippet

def code_generate_then_execute(instruction, code_input):
    # output = openai.Edit.create(
    # model="code-davinci-edit-001",
    # input="x = " + code_input,
    # instruction="Python code for " + instruction,
    # temperature=0,
    # top_p=1
    # )
    # code_snippet = output.choices[0].text

    code_snippet, generate_details = code_generate(instruction=instruction, code_input=code_input)
    time.sleep(2)
    # result = subprocess.run([sys.executable, "-c", code_snippet], capture_output=True, text=True)
    # if result.stderr == "":
    #     return result.stdout, code_snippet
    result = code_execute(code_snippet=code_snippet)
    return result


def code_error(code_snippet, code_input):
    # Command instruction (if its code) starts in the next line.
    try:
        code_snippet = code_snippet.split("\n", 1)[1]
        result = subprocess.run([sys.executable, "-c", code_snippet], capture_output=True, text=True, timeout=60)
        if result.stderr != "":
            return result.stderr, code_snippet
    except:
        return None, code_snippet
    

def code_edit(instruction, code_input):
    comment = "# " + instruction + "\n"
    response = openai.Completion.create(
    # model="code-davinci-002",
    model="davinci-codex-002-msft",
    prompt=comment,
    temperature=0.0,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    stop=["\n"]
    )
    
    code_snippet = response.choices[0].text
    return comment + code_snippet, comment

def code_generate_then_lookup(instruction, code_input):

    code_snippet, generate_details = code_generate(instruction=instruction, code_input=code_input)
    time.sleep(2)
    result, execute_snippet = code_execute(code_snippet=code_snippet)
    d = enchant.Dict("en_US")
    try:
        word_list = [''.join(chat_list) for chat_list in ast.literal_eval(result)]
        valid_list = []
        for word in word_list:
            if d.check(word):
                valid_list.append(word)
        return simplify_ans(valid_list), execute_snippet
    except: 
        return None, execute_snippet

def lookup(word_list):
    import enchant
    d = enchant.Dict("en_US")
    valid_list = []
    for word in word_list:
        if d.check(word):
            valid_list.append(word)
    return valid_list

def arithmetic(equations, previous_input):
    # collect all outputs of arithmetic and run python to execute them
    result = subprocess.run([sys.executable, "-c", "\n".join(equations)], capture_output=True, text=True)
    return result, None

def generate(prompt, previous_input):
    return gpt3(prompt)[0], None


class Interpreter():
    def __init__():
        pass

'''
class TopDownVisitor(Interpreter):
    def __init__(self, model_name="text-davinci-002"):
        self.built_ins = {
            "[generate]": generate,
            "[search]": search,
            "[code generate]": code_generate,
            "[code execute]": code_execute,
            "[string edit]" : code_generate_then_execute,
            # "[arithmetic]" : arithmetic,
            "[generate python code]" : code_generate,
        }

        self.program_completer = OpenAIModel(model=model_name,  max_length=500, quote='', n=1)

    def syntax_check(self, program):
        # Look for programs with EOC ending and final answer in syntax.
        return "[EOQ]\nAns:" in program

    def batch_visit(self, prefixes, programs):
        answers = []
        for prefix, program in tqdm(zip(prefixes, programs)):
            answers.append(self.visit(prefix, program))
        return answers

    def check_builtin(self, command):
        # Some multiple commands are mapped to the same built-in affordances.
        processed_command = command
        if command in self.built_ins:
            return self.built_ins[processed_command] 
        # What to do when no built-infunction is triggered.

    def complete_program(self, prefix, program):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        continuation = program
        runs = 5
        while not self.syntax_check(continuation) and runs > 0:
            continuation = self.program_completer(prefix + program)[0]
            runs -= 1
        return program + continuation

    def rerun_program(self, prefix, prev_command_list, current_command_output):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        program_prefix = prefix + "\n".join([Command.convert_to_nlprogram(i+1, command) for i, command in enumerate(prev_command_list[:-1])])
        # TODO: Add feature for multiline
        program_prefix += "\n" + Command.convert_to_nlprogram(len(prev_command_list), prev_command_list[-1], input_only=True) + "\n#{0}: {1}".format(len(prev_command_list), current_command_output)
        # Replace last line with corrected input, if necessary.
        continuation = self.program_completer(program_prefix)[0]
        return program_prefix, continuation

    def visit(self, prefix, program):
        
        
        program_ends = self.syntax_check(program)
        program = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1] + program
        # If program does not end correctly, rerun GPT-3 further to end it. 
        if program_ends:
            parsed_program =  parse_program(program)
        else:
            program = self.complete_program(prefix, program)
            program_ends = self.syntax_check(program)
            if not program_ends:
                # After one attempt at completing program, give up checking for affordance
                return program
            parsed_program =  parse_program(program)

        previous_input = None
        i = 0
        input_node = parsed_program.input_node
        command_node_list = parsed_program.node_list
        answer_node = parsed_program.answer_node
        stack_trace = []
        while command_node_list[i].command_name != "[EOQ]":
            
            node = command_node_list[i]
            command_name = node.command_name
            command_input = node.command_input
            command_output = node.command_output
            # Check affordance list to run.
            affordance = self.check_builtin(command=command_name)
            
            if affordance and previous_input: # previous_input so code generate is hard, But search and code_execute is not, overrride.
                affordance_output, affordance_details = affordance(command_input, previous_input)
                if affordance_output:
                    # affordance_output and command_output (i.e. GPT-3) need to be interpolated appropriately.
                    # For code executions
                    command_output = affordance_output
                    # For Search (concatenate the GPT_3 output with retrieval output)
                
                # Run GPT-3 again.
                program_prefix, rerun_program = self.rerun_program(prefix, command_node_list[:i+1], command_output)

                
                # Replace nodes i+1 though N (again complication is that this should be done EOC i.e. till the program is fully generated.)
                new_program = program_prefix + rerun_program
                new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]
                try:
                    parsed_rerun_program = parse_program(new_program)
                except:
                    pdb.set_trace()

                parsed_program = parsed_rerun_program
                program = new_program


                # for j in range(i+1, len(command_node_list)):
                #     command_node_list[j] = parsed_rerun_program.node_list[j]
                command_node_list = command_node_list[:i+1] + parsed_rerun_program.node_list[i+1:]

                answer_node = parsed_rerun_program.answer_node

                stack_trace.append(StacktraceItem(node, affordance, affordance_output, affordance_details, new_program, parsed_rerun_program))
            
            # Output of the current command becomes the input for the next round.
            previous_input = command_output
            i += 1

        # Since run is complete, The final answer in parsed_rerun_program is returned
        # Selectively, we can also return the entire program
        return program
'''

class TopDownVisitorBeta(Interpreter):
    def __init__(self, model_name="text-davinci-002", temperature=0.3, rerun=True, exclude_list=[],
                 get_log=None, echo=None):
        self.built_ins = {
            # "[generate]": generate,
            "[search]": google_search,
            "[code generate]": code_generate,
            "[code execute]": code_execute,
            "[code edit]": code_edit,
            "[string edit]": code_generate_then_execute,
            "[string index]": code_generate_then_execute,
            "[string permutation]": code_generate_then_lookup,
            # "[arithmetic]" : arithmetic,
            "[generate python code]": code_generate,
        }
        self.built_in_with_explanation = {
            # "[generate]": generate,
            "[search]": "Uses google search api to retrieve context from internet",
            "[code generate]": "generate code to solve the problem",
            "[code execute]": "execute the generated code and obtain the result",
            "[code edit]": "edit code so that it is free of bug",
            "[string operations]": "generate code to edit string positions",
            # "[string index]": "generate relevant code for string operations",
            # "[string permutation]": "generate relevant code for string operations",
            "[arithmetic]" : "Use arithmetic operations in this step for numeric calculation",
            # "[generate python code]": "generate code to solve the problem",
        }
        if len(exclude_list):
            for key in exclude_list:
                del self.built_ins[key]
        self.keyword_map = {
            "[permutation]": "[string permutation]",
            "[permute]": "[string permutation]",
            "[string permute]": "[string permutation]",
            "[execute]": "[code execute]"}

        self.execution_details = []
        self.rerun = rerun
        self.get_log = get_log
        self.echo = echo
        # make model return logprobs if requested
        self.program_completer = OpenAIModel(model=model_name, temperature=temperature, max_length=500, quote='---', n=1,
                                             logprobs=1 if self.get_log else None, echo=self.echo)
        # else:
        #     self.program_completer = OpenAIModel(model=model_name, temperature=temperature, max_length=500, quote='---', n=1)
    def add_self_eval(self, self_eval):
        self.program_completer.add_sys_prompt(self_eval)

    def syntax_check(self, program):
        # Look for programs with EOC ending and final answer in syntax.
        return "[EOQ]\nAns:" in program

    def batch_visit(self, prefixes, programs):
        answers = []
        for prefix, program in zip(prefixes, programs):
            answers.append(self.visit(prefix, program))
        return answers

    def batch_visit_with_prob(self, prefixes, programs, call):
        answers = []
        new_logprobs = []
        for prefix, program in zip(prefixes, programs):
            ans, new_logprobs = self.visit(prefix, program, init_log=call)
            answers.append(ans)

        # print(answers)

        return answers, new_logprobs

    def check_builtin(self, command):
        # Some multiple commands are mapped to the same built-in affordances.
        processed_command = command
        if command in self.built_ins:
            return self.built_ins[processed_command]
        elif command in self.keyword_map:
            return self.built_ins[self.keyword_map[processed_command]]
        # What to do when no built-infunction is triggered.

    def shorten_prefix(self, prefix, to_remove=1):
        # Program prefix contains other demonstrations, along with Q1:
        individual_programs = prefix.split("\n----\n")
        
        # to Remove: Number of programs to remove 
        # If they are organized by task relevance probably best to get rif of the latter ones.
        shifted_programs = individual_programs[:-(to_remove+1)] + [individual_programs[-1]]
        new_prefix = "\n----\n".join(shifted_programs)
        
        return new_prefix

    def complete_program(self, prefix, program):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.
        continuation = program
        runs = 5
        # prefix = self.shorten_prefix(prefix, 1)
        logprobs = None
        while not self.syntax_check(continuation) and runs > 0:
            # TODO: Explicitly, check if the program ends in answers or questions and accordingly change the separator.
            separator = "\nQ"
            continuation += separator
            # logprob has different format
            if self.get_log:
                completion = self.program_completer(prefix + continuation)
                # format between instruct and chat gpt
                try:
                    continuation = completion['choices'][0]['text']
                except:
                    continuation = completion['choices'][0]["message"]["content"]
                logprobs = completion['choices'][0]['logprobs']
            else:
                continuation = self.program_completer(prefix + continuation)[0]
            continuation = program + separator + continuation
            runs -= 1
            # prefix = self.shorten_prefix(prefix, 1)

        # needs rerun initial logprobs
        if self.get_log:
            return continuation, logprobs
        return continuation

    def rerun_program(self, prefix, prev_command_list, current_command_output):
        # Check for out-of-window errors and shift window to reduce the number of programs shown.


        # print("\n".join([Command.convert_to_nlprogram(i+1, command) for i, command in enumerate(prev_command_list[:-1])]))

        program_prefix = prefix + "\n".join([Command.convert_to_nlprogram(i+1, command) for i, command in enumerate(prev_command_list[:-1])])


        # TODO: Add feature for multiline
        output_sep = "\n" if "\n" in current_command_output else " "
        program_sep = "" if len(prev_command_list) == 1 else "\n"
        program_prefix += program_sep + Command.convert_to_nlprogram(len(prev_command_list), prev_command_list[-1], input_only=True) + "\n#{0}:{1}{2}\n".format(len(prev_command_list), output_sep, current_command_output.strip())



        # Replace last line with corrected input, if necessary.
        # logprob has different format, note here we need logprobs (complete program doesn't need logprobs)
        if self.get_log:
            continuation = self.program_completer(program_prefix)
        else:
            continuation = self.program_completer(program_prefix)[0]

        # print('-----')
        # print(continuation['choices'][0]['text'])
        # print('-----')
        return program_prefix, continuation

    def rerun_answer(self, prefix):
        # logprob has different format, note here we need logprobs (complete program doesn't need logprobs)
        if self.get_log:
            continuation = self.program_completer(prefix)
        else:
            continuation = self.program_completer(prefix)[0]
        return continuation

    # takes in dict and return a list of tool token probs
    def extract_tool_token_logprob(self, call):
        scores = []  # list of tool token logprobs
        # print(call)
        # different logprob struct in instruct and chat gpt
        try:
            logprobs = np.array(call['token_logprobs'])
        except:
            call = {
                'token_logprobs': [d["logprob"] for d in call["content"]],
                'tokens': [d["token"] for d in call["content"]],
            }
            logprobs = np.array(call['token_logprobs'])
        # filter out the tool calling segment
        indices = []  # nested list with tool token indice(s)
        tool_token_ind = []
        is_in = False
        for i, token in enumerate(call['tokens']):
            if '[' in token:
                is_in = True
            elif ']' in token:
                is_in = False
                if tool_token_ind:
                    indices.append(tool_token_ind)
                tool_token_ind = []
            elif is_in:
                res = re.match(r"[0-9]+", token)
                if res is None:
                    tool_token_ind.append(i)
        for i in indices:
            # # filter out the EOQ token
            # token_array = np.array(call['tokens'])
            # t = ''.join(token_array[i])
            # print(t)
            # if 'EOQ' not in t:
            scores.append(np.average(logprobs[i]))

        return scores


    # Re-ranking of all built_ins when using tools
    '''
    input: command list, list_of_tools
    output: selected option at reasoning step i, [optional] adj_conf of the option
    '''
    def rerank_tool(self, prefix, cur_command_list, return_conf=False):
        tool_list = [':'.join([k, v]) for (k, v) in self.built_in_with_explanation.items()] + ["[No tool]: select this option if there is no need to use tool at this step"]
        question = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1]
        # Option 1: for loop to gather conf scores
        rerank_prompt = """Provide your confidence of using the option described below given the context of a reasoning process.
Note: The confidence score indicates the degree of certainty you have about your answer and is often represented as a number from 0 to 100.
----
Reasoning context: %s
Option: %s
Confidence (a number between 0-100, no explanation): 
        """

        for tool in tool_list:
            prompt = rerank_prompt % (question + "\n".join([Command.convert_to_nlprogram_no_conf(i+1, command) for i, command in enumerate(cur_command_list)]) if len(cur_command_list) > 0 else question, tool)
            # print(f'This is the self report conf prompt on each tool: \n----\n {prompt}')
            conf = self.program_completer(prompt)['choices'][0]['text']
            print(f"{tool}\n Conf: {conf}")


        # Option 2: one call to get self-reported conf scores
        print('\n----\n')
        rerank_prompt_one_call = """You are given a list of available options to choose from as the next step to continue the reasoning of a complex question. Attach the options with the confidence score of using each option. Put your results in a python dictionary.\nNote: The confidence score indicates the degree of certainty you have about your selection and is often represented as a number from 0 to 100.
----
Reasoning context: %s
List of options: %s
Answer (in python dict format): """

        prompt = rerank_prompt_one_call % (
                                           question + "\n".join([Command.convert_to_nlprogram_no_conf(i+1, command) for i, command in enumerate(cur_command_list)]) if len(cur_command_list) > 0 else question,
                                           '\n'.join(tool_list)
                                           )

        # print(f"This is the prompt conf prompt to rerank tool:\n----\n {prompt}")

        ranking = self.program_completer(prompt)['choices'][0]['text']

        print(f"This is the tool ranking from one call:\n {ranking}")


    def visit(self, prefix, program, init_log=None):
        
        program_ends = self.syntax_check(program)
        program = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1] + program
        cont_log = None

        # check the program
        # print("this is the original program")
        # print(program)

        # If program does not end correctly, rerun GPT-3 further to end it. 
        if program_ends:
            try:
                parsed_program = parse_program(program)

            except Exception as e:
                # If the initial program is not parsed even if it has an ending, return as is
                print(f"program parsing failed! {e}")
                # check whether to return logprob
                if self.get_log:
                    return program, []
                return program
        else:
            print("Program ended incorrectly, give it another chance...")
            if self.get_log:
                new_program, cont_log = self.complete_program(prefix, program)
            else:
                new_program = self.complete_program(prefix, program)
            program_ends = self.syntax_check(new_program)
            if not program_ends:
                # After one attempt at completing program, give up checking for affordance
                print("still fails!")
                # check whether to return logprob
                if self.get_log:
                    return program, []
                return program
            try:
                parsed_program = parse_program(new_program)
            except:
                print("program parsing failed in second chance!")
                # check whether to return logprob
                if self.get_log:
                    return program, []
                return program

            # print(f"This is the new program: {new_program}")

        # get tool prob from initial call
        if self.get_log:
            # get initial logprobs from tools
            tool_scores = self.extract_tool_token_logprob(init_log)
            if cont_log:
                tool_scores += self.extract_tool_token_logprob(cont_log)

        previous_input = None
        i = 0
        input_node = parsed_program.input_node
        command_node_list = parsed_program.node_list
        answer_node = parsed_program.answer_node
        stack_trace = []
        logprobs = []

        if not program_ends:
            # After one attempt at completing program, give up checking for affordance
            # check whether to return logprob
            if self.get_log:
                return program, []
            return program

        previous_input = input_node.text

        while (i < len(command_node_list)) and (command_node_list[i].command_name != "[EOQ]") and (i < 10):
            # print(f"command list: {[c.command_name for c in command_node_list]}")
            # print(f"checking command {i}: {command_node_list[i].command_name}")

            # Optional module to rerank tools
            # self.rerank_tool(prefix, command_node_list[:i])

            node = command_node_list[i]

            # edit the output conf_socre

            # print(f"current cmd length: {len(command_node_list)}")
            # print(f"current score list length: {len(tool_scores)}")
            if self.get_log:
                try:
                    command_node_list[i].command_conf = f'[{int(100*np.e**tool_scores[i])}]'
                except:
                    print("Conf score substitution failed. Moving on...")

            command_name = node.command_name
            command_input = node.command_input
            command_output = node.command_output

            # print(f"this is the node: {node} \n")
            # print(command_name, command_input, command_output, node.command_conf)

            # Check affordance list to run.
            affordance = self.check_builtin(command=command_name)

            # if not affordance:
            #     print("Command not in tool list")
            #     print(f"Using {command_name} now...")
            #     print(f"Output: {command_output}")

            if affordance:

                # print(f"Using {command_name} now...")

                affordance_output, affordance_details = affordance(command_input, previous_input)

                # print(f"The affordance output: {affordance_output}")

                if affordance_output:
                    # print(f"This is {command_name} output: {affordance_output}")

                    # affordance_output and command_output (i.e. GPT-3) need to be interpolated appropriately.
                    # For Search (concatenate the GPT_3 output with retrieval output)
                    if command_name == "[search]":
                        command_output = " " + affordance_output
                    else:
                        command_output = affordance_output

                if self.rerun:
                    # Run GPT-3 again (technically only if the execution is different)
                    program_prefix, rerun_program = self.rerun_program(prefix + "\nQ1:", command_node_list[:i+1], command_output)

                    # for logprobs, rerun_program returns the full resp object
                    if self.get_log:
                        rerun_logprobs = rerun_program['choices'][0]['logprobs'] # logprob dict object with tool edited logprobs
                        logprobs.append(rerun_logprobs)
                        # format between instruct and chat gpt
                        try:
                            rerun_program = rerun_program['choices'][0]['text']
                        except:
                            rerun_program = rerun_program['choices'][0]["message"]["content"]
                        tool_scores = tool_scores[:i+1] + self.extract_tool_token_logprob(rerun_logprobs) # slice and extend the tool scores

                        # print(f"This is rerun program: {rerun_program}")
                        # print(f"This is the rerun tool scores: {tool_scores}")

                    
                    # Replace nodes i+1 though N (again complication is that this should be done EOC i.e. till the program is fully generated.)
                    new_program = program_prefix + rerun_program
                    new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]

                    # print(f"this is the new program:\n {new_program}")


                    # Not so sure what is the meaning of this step
                    if self.get_log:
                        new_program, rerun_cont_log = self.complete_program(prefix, new_program)
                    else:
                        new_program = self.complete_program(prefix, new_program)

                    # print(f"----\nthis is the new program after completion:\n {new_program}\n----")


                    # new_program is now a complete program
                    try:
                        parsed_rerun_program = parse_program(new_program)
                        # print('tool use after rerun success!')
                    except:
                        # Everytime the rerun fails to generate a legitimate program, its unsafe to assume that it could replace the rest of the original program. 
                        # Best course of action is to exit with the best new_program
                        program = new_program
                        break

                    parsed_program = parsed_rerun_program
                    program = new_program
                    # command_node_list = command_node_list[:i+1] + parsed_rerun_program.node_list[i+1:]
                    command_node_list = parsed_rerun_program.node_list
                    answer_node = parsed_rerun_program.answer_node

                    stack_trace.append(StacktraceItem(node, affordance, affordance_output, affordance_details, new_program, parsed_rerun_program))
                else:
                    # Run GPT-3 again (technically only if the execution is different)
                    # program_prefix, rerun_program = self.rerun_program(prefix, command_node_list[:i+1], command_output)
                    prev_command_list = command_node_list[:i+1]
                    program_prefix = prefix + "\n".join([Command.convert_to_nlprogram(j+1, command) for j, command in enumerate(prev_command_list[:-1])])
                    # TODO: Add feature for multiline
                    output_sep = "\n" if "\n" in command_output else " "
                    program_sep = "" if len(prev_command_list) == 1 else "\n"

                    # Change format to accommondate conf score as well!
                    program_prefix += program_sep + Command.convert_to_nlprogram(len(prev_command_list), prev_command_list[-1], input_only=True) + "\n#{0}:{1}{2}\n".format(len(prev_command_list), output_sep, command_output.strip())
                    program_prefix += "\n".join([Command.convert_to_nlprogram(j+len(prev_command_list)+1, command) for j, command in enumerate(command_node_list[i+1:])])
                    new_program = program_prefix
                    # parsed program does not change since the future nodes are not updated.
                    # However, final answer can change due to selective replacement of the intermediate steps.
                    continuation = self.rerun_answer(program_prefix)
                    # for logprobs, rerun_answer returns the full resp object
                    if self.get_log:
                        logprobs.append(continuation['choices'][0]['logprobs'])
                        continuation = continuation['choices'][0]['text']
                    new_program += continuation
                    new_program = new_program.rsplit("----\n", 1)[1].split("\n", 1)[1]
                    stack_trace.append(StacktraceItem(node, affordance, affordance_output, affordance_details, new_program, parsed_program))

            # Update conf_score when no match affordance found
            if (not affordance) and self.get_log:
                # program = task question chunk + answer steps in Command obj.
                program = prefix.rsplit("----\n", 1)[1].split("\n", 1)[1] + "\nQ1:" + "\n".join([Command.convert_to_nlprogram(j+1, command) for j, command in enumerate(command_node_list[:-1])])

                ### Needs to try afterwards
                if self.get_log:
                    new_program, new_logprob = self.complete_program(prefix, program)
                else:
                    new_program = self.complete_program(prefix, program)
                program = new_program
                # print(program)





                # Output of the current command becomes the input for the next round.
            previous_input = command_output
            i += 1
        # Since run is complete, The final answer in parsed_rerun_program is returned
        # Selectively, we can also return the entire program
        self.execution_details.append(stack_trace)
        # check whether to return logprobs
        if self.get_log:
            return program, logprobs
        return program


